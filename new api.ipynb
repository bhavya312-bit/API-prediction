{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HcmUs7S1B2JL",
    "outputId": "8aa4d4ee-acfd-42e6-be74-98ba6e8237e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-3177eb6024a6>:157: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
      "<ipython-input-6-3177eb6024a6>:161: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
      "<ipython-input-6-3177eb6024a6>:165: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
      "ERROR:root:DateTime conversion failed. Sample date format: 01-02-2025 07:21\n",
      "ERROR:root:Error processing A9: DateTime conversion failed: time data \"13-02-2025 00:40\" doesn't match format \"%m-%d-%Y %H:%M\", at position 182. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "ERROR:root:Failed to train model for A9: DateTime conversion failed: time data \"13-02-2025 00:40\" doesn't match format \"%m-%d-%Y %H:%M\", at position 182. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "ERROR:root:DateTime conversion failed. Sample date format: 01-02-2025 01:29\n",
      "ERROR:root:Error processing A2: DateTime conversion failed: time data \"13-02-2025 00:22\" doesn't match format \"%m-%d-%Y %H:%M\", at position 180. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "ERROR:root:Failed to train model for A2: DateTime conversion failed: time data \"13-02-2025 00:22\" doesn't match format \"%m-%d-%Y %H:%M\", at position 180. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "ERROR:root:DateTime conversion failed. Sample date format: 01-02-2025 02:39\n",
      "ERROR:root:Error processing A7: DateTime conversion failed: time data \"13-02-2025 03:24\" doesn't match format \"%m-%d-%Y %H:%M\", at position 202. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "ERROR:root:Failed to train model for A7: DateTime conversion failed: time data \"13-02-2025 03:24\" doesn't match format \"%m-%d-%Y %H:%M\", at position 202. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://d6ec4a974eb03a92a6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d6ec4a974eb03a92a6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def check_data_requirements(df):\n",
    "    \"\"\"\n",
    "    Validate the input data meets all requirements\n",
    "    \"\"\"\n",
    "    required_columns = ['Time of Call', 'API Code']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def preprocess_and_train(api_file, api_code):\n",
    "    \"\"\"\n",
    "    Preprocess data and train ARIMA model with extended error handling\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting preprocessing for {api_code}\")\n",
    "\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(api_file):\n",
    "            raise FileNotFoundError(f\"File not found: {api_file}\")\n",
    "\n",
    "        # Load data with explicit encoding and error handling\n",
    "        try:\n",
    "            api_data = pd.read_csv(api_file, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            api_data = pd.read_csv(api_file, encoding='latin1')\n",
    "\n",
    "        logging.info(f\"Successfully loaded data for {api_code}\")\n",
    "\n",
    "        # Validate data\n",
    "        check_data_requirements(api_data)\n",
    "\n",
    "        # Convert to datetime with error handling\n",
    "        try:\n",
    "            api_data['Time of Call'] = pd.to_datetime(api_data['Time of Call'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"DateTime conversion failed. Sample date format: {api_data['Time of Call'].iloc[0]}\")\n",
    "            raise ValueError(f\"DateTime conversion failed: {str(e)}\")\n",
    "\n",
    "        api_data.set_index('Time of Call', inplace=True)\n",
    "\n",
    "        # Resample data\n",
    "        daily_calls = api_data['API Code'].resample('D').count()\n",
    "\n",
    "        if len(daily_calls) < 10:  # Minimum data requirement\n",
    "            raise ValueError(\"Insufficient data points for prediction\")\n",
    "\n",
    "        # Split data\n",
    "        train_size = int(len(daily_calls) * 0.8)\n",
    "        train, test = daily_calls[:train_size], daily_calls[train_size:]\n",
    "\n",
    "        logging.info(f\"Training ARIMA model for {api_code}\")\n",
    "\n",
    "        # Train ARIMA model\n",
    "        arima_model = ARIMA(train, order=(5,1,0))\n",
    "        fitted_model = arima_model.fit()\n",
    "\n",
    "        # Make predictions for test set\n",
    "        arima_forecast = fitted_model.forecast(steps=len(test))\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(test, arima_forecast)\n",
    "        mse = mean_squared_error(test, arima_forecast)\n",
    "\n",
    "        # Save model\n",
    "        model_filename = f\"{api_code}_best_model.pkl\"\n",
    "        joblib.dump(fitted_model, model_filename)\n",
    "        logging.info(f\"Model saved as {model_filename}\")\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'metrics': {\n",
    "                'MAE': mae,\n",
    "                'MSE': mse\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {api_code}: {str(e)}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def predict_api_calls(api_choice, days):\n",
    "    \"\"\"\n",
    "    Make predictions with enhanced error handling\n",
    "    \"\"\"\n",
    "    logging.info(f\"Making predictions for {api_choice} for {days} days\")\n",
    "\n",
    "    try:\n",
    "        model_path = f\"{api_choice}_best_model.pkl\"\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            return None, f\"Error: Model file not found for {api_choice}\"\n",
    "\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        future_dates = pd.date_range(\n",
    "            start=datetime.today(),\n",
    "            periods=days,\n",
    "            freq='D'\n",
    "        )\n",
    "\n",
    "        predictions = model.forecast(steps=days)\n",
    "\n",
    "        # Ensure predictions are non-negative\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            \"Date\": future_dates,\n",
    "            \"Predicted_Calls\": predictions.round(0)  # Round to whole numbers\n",
    "        })\n",
    "\n",
    "        # Create plot\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(results[\"Date\"], results[\"Predicted_Calls\"])\n",
    "        plt.title(f\"{api_choice} API Call Predictions\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Predicted Calls\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return plt.gcf(), results.to_string()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Prediction error: {str(e)}\")\n",
    "        return None, f\"Error during prediction: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function with sample data handling\n",
    "    \"\"\"\n",
    "    # Check if sample data exists, if not, create it\n",
    "    sample_data = {\n",
    "        'A9.csv': pd.DataFrame({\n",
    "            'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
    "            'API Code': np.random.randint(1, 100, 100)\n",
    "        }),\n",
    "        'A2.csv': pd.DataFrame({\n",
    "            'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
    "            'API Code': np.random.randint(1, 100, 100)\n",
    "        }),\n",
    "        'A7.csv': pd.DataFrame({\n",
    "            'Time of Call': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
    "            'API Code': np.random.randint(1, 100, 100)\n",
    "        })\n",
    "    }\n",
    "\n",
    "    # Save sample data if real data doesn't exist\n",
    "    for file_name, data in sample_data.items():\n",
    "        if not os.path.exists(file_name):\n",
    "            logging.info(f\"Creating sample data file: {file_name}\")\n",
    "            data.to_csv(file_name, index=False)\n",
    "\n",
    "    # Train models\n",
    "    api_files = {'A9.csv': 'A9', 'A2.csv': 'A2', 'A7.csv': 'A7'}\n",
    "\n",
    "    for file, api_code in api_files.items():\n",
    "        result = preprocess_and_train(file, api_code)\n",
    "        if result['success']:\n",
    "            logging.info(f\"Successfully trained model for {api_code}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to train model for {api_code}: {result['error']}\")\n",
    "\n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=predict_api_calls,\n",
    "        inputs=[\n",
    "            gr.Dropdown(choices=[\"A9\", \"A2\", \"A7\"], label=\"Choose API\"),\n",
    "            gr.Slider(minimum=1, maximum=30, step=1, value=7, label=\"Days to Predict\")\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Plot(label=\"Prediction Plot\"),\n",
    "            gr.Textbox(label=\"Detailed Results\", lines=10)\n",
    "        ],\n",
    "        title=\"API Call Prediction\",\n",
    "        description=\"Predict API calls for the next N days\"\n",
    "    )\n",
    "\n",
    "    interface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
